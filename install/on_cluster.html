<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using MALA on HPC systems &mdash; Materials Learning Algorithms (MALA)  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/mala_favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Successfully tested on" href="tested_systems.html" />
    <link rel="prev" title="External modules" href="external_modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/mala_horizontal_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../installation.html">Installation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="README.html">Installation of MALA</a></li>
<li class="toctree-l2"><a class="reference internal" href="external_modules.html">External modules</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using MALA on HPC systems</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#installation-tips-tricks">Installation: tips &amp; tricks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hemera5-hzdr">Hemera5  (HZDR)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#summit-cluster-ornl">Summit cluster (ORNL)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#sql">SQL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorboard">Tensorboard</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Hemera5 (HZDR)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#horovod">Horovod</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">Hemera5 (HZDR)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tested_systems.html">Successfully tested on</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/modules.html">API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CONTRIBUTE.html">Contributing to MALA</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Materials Learning Algorithms (MALA)</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../installation.html">Installation</a> &raquo;</li>
      <li>Using MALA on HPC systems</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com//mala-project/mala/blob/develop/docs/source/install/on_cluster.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="using-mala-on-hpc-systems">
<h1>Using MALA on HPC systems<a class="headerlink" href="#using-mala-on-hpc-systems" title="Permalink to this headline"></a></h1>
<p>Generally, MALA can be used on an HPC system like any other regular python
module, with one exception being the setup of
<a class="reference internal" href="external_modules.html"><span class="doc">external modules</span></a>. Detailed here are some other
aspects of the MALA workflow for which the setup on HPC systems is not
immediately clear.
Some of the following information targets user of Hemera5, the main cluster
of the
Helmholtz-Zentrum Dresden-Rossendorf and Summit, one of the clusters at Oak
Ridge national laboratories.</p>
<div class="section" id="installation-tips-tricks">
<h2>Installation: tips &amp; tricks<a class="headerlink" href="#installation-tips-tricks" title="Permalink to this headline"></a></h2>
<div class="section" id="hemera5-hzdr">
<h3>Hemera5  (HZDR)<a class="headerlink" href="#hemera5-hzdr" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Tested by: Lenz Fiedler, 23.03.2022</p></li>
<li><p>Scope of test: mala, torch</p></li>
<li><p>Test log:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Create a conda environment:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">create</span> <span class="pre">--name</span> <span class="pre">mala_train</span> <span class="pre">python=3.8.5</span></code></p></li>
</ul>
</div></blockquote>
</li>
<li><p>Install Pytorch. I load CUDA for this.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">cuda/10.2</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torch</span></code></p></li>
</ul>
</div></blockquote>
</li>
<li><p>Then install <cite>mala</cite> via</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-e</span> <span class="pre">.</span></code></p></li>
</ul>
</div></blockquote>
</li>
</ol>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="summit-cluster-ornl">
<h3>Summit cluster (ORNL)<a class="headerlink" href="#summit-cluster-ornl" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Tested by: Vladyslav Oles, 16.04.2021</p></li>
<li><p>Scope of test: mala, torch, horovod, oapackage</p></li>
<li><p>Test log:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Install a current version of SWIG (instead of Summit’s 2.0.10) as per <a class="reference external" href="http://www.swig.org/svn.html">http://www.swig.org/svn.html</a>:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">https://github.com/swig/swig.git</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">swig</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./autogen.sh</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./configure</span> <span class="pre">--prefix=/.../swig</span></code> (replace <code class="docutils literal notranslate"><span class="pre">...</span></code> with absolute path to <code class="docutils literal notranslate"><span class="pre">swig</span></code> directory)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">make</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">install</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PATH=/.../swig/bin:$PATH</span></code> (replace <code class="docutils literal notranslate"><span class="pre">...</span></code> with actual absolute path to <code class="docutils literal notranslate"><span class="pre">swig</span></code> directory)</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Clone conda environment with pytorch and horovod, and add oapackage to it:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">open-ce/1.1.3-py38-0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">create</span> <span class="pre">--name</span> <span class="pre">mala-opence</span> <span class="pre">--clone</span> <span class="pre">open-ce-1.1.3-py38-0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">mala-opence</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">gxx_linux-ppc64le</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">oapackage</span></code></p></li>
</ul>
</div></blockquote>
</li>
<li><p>Install MALA (from the directory with cloned MALA repository):</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-e</span> <span class="pre">.</span></code></p></li>
</ul>
</div></blockquote>
</li>
</ol>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="sql">
<h2>SQL<a class="headerlink" href="#sql" title="Permalink to this headline"></a></h2>
<p>To run extended hyperparameter studies using Optuna, an SQL server is
needed that coordinates trials between different jobs. The MALA developers
recommend using PostgreSQL. Once PostgreSQL is set up, the server has to be
started via a separate job in order for the hyperparameter study to run.
To set up PostgreSQL, use the following steps.</p>
<ol class="arabic">
<li><p>Install postgres (e.g. in a conda environment)</p></li>
<li><p>Initialize a postgres server configuration somewhere in your home directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/a/suitable/location/
$<span class="w"> </span>initdb<span class="w"> </span>-D<span class="w"> </span>YOUR_SERVER_NAME
</pre></div>
</div>
</li>
<li><p>Edit the configuration files of this server:</p>
<blockquote>
<div><ul class="simple">
<li><p>In <code class="docutils literal notranslate"><span class="pre">postgres_local/postgresql.conf</span></code> add/change the line containing listen_adresses so that it reads:
<code class="docutils literal notranslate"><span class="pre">listen_addresses</span> <span class="pre">=</span> <span class="pre">'*'</span></code></p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">pg_hba.conf</span></code>, under <code class="docutils literal notranslate"><span class="pre">IPv$</span></code> local connections add the line:
<code class="docutils literal notranslate"><span class="pre">host</span>&#160;&#160;&#160; <span class="pre">all</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">all</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">0.0.0.0/0</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">trust</span></code></p></li>
</ul>
</div></blockquote>
</li>
<li><p>Start the postgres server:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>pg_ctl<span class="w"> </span>-D<span class="w"> </span>postgres_local<span class="w"> </span>-l<span class="w"> </span>logfile<span class="w"> </span>start
</pre></div>
</div>
</li>
<li><p>Create the “username database”</p>
<blockquote>
<div><ul class="simple">
<li><p>This is needed in order to access the psql interface (for maintenance): <code class="docutils literal notranslate"><span class="pre">createdb</span></code></p></li>
</ul>
</div></blockquote>
</li>
<li><p>Create a database for your hyperparameter optimizations: <code class="docutils literal notranslate"><span class="pre">createdb</span> <span class="pre">YOUR_DATABASE_NAME</span></code></p></li>
<li><p>Host a postgres server via a compute job:</p>
<blockquote>
<div><ul>
<li><p>You can/should also create a database for optuna, so you can create a study from within Python</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/a/suitable/location/
pg_ctl<span class="w"> </span>-D<span class="w"> </span>A_DESCRIPTIVE_SERVER_NAME<span class="w"> </span>-l<span class="w"> </span>logfile<span class="w"> </span>start
createdb<span class="w"> </span>YOUR_SERVER_NAME

<span class="k">while</span><span class="w"> </span>true<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">  </span>pg_ctl<span class="w"> </span>-D<span class="w"> </span>postgres_local<span class="w"> </span>status
<span class="w">  </span>sleep<span class="w"> </span>60m
<span class="k">done</span>
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</li>
<li><p>MALA/Optuna can now connect to this database via</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="o">.</span><span class="n">hyperparameters</span><span class="o">.</span><span class="n">rdb_storage</span> <span class="o">=</span> <span class="s2">&quot;postgresql://YOUR_USER_NAME@YOUR_COMPUTE_NODE/YOUR_DATABASE_NAME&quot;</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>[optional] To increase the maximum number of connections to PostgreSQL (default is 100), go to <code class="docutils literal notranslate"><span class="pre">a/suitable/location/postgres_local/postgresql.conf</span></code> and change the value of <code class="docutils literal notranslate"><span class="pre">max_connections</span></code>. In addition, the value of <code class="docutils literal notranslate"><span class="pre">shared_buffers</span></code> is recommended to be set to about 10% of RAM available on a compute node. Note that the actual number of connections exceeds the number of workers, likely because the workers establish new connections every time they communicate with DB and idle connections not dying out by default.</p></li>
</ol>
</div>
<div class="section" id="tensorboard">
<h2>Tensorboard<a class="headerlink" href="#tensorboard" title="Permalink to this headline"></a></h2>
<p>MALA visualization via tensorboard works by simply saving the logs into a
user specified directory. There are two ways to use visualization while working
on an HPC cluster:</p>
<ol class="arabic simple">
<li><p>Copy the visualization directory onto your local machine</p></li>
<li><p>Mount the folder on your HPC cluster to your local machine (cluster specific)</p></li>
</ol>
<div class="section" id="id1">
<h3>Hemera5 (HZDR)<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>If you are using the Hemera5 cluster, you can use the following informatiomn
to mount the folders to your local machine.</p>
<ul>
<li><p>You can find information on how to mount Hemera onto a local device here:</p>
<p>&lt;<a class="reference external" href="https://fwcc.pages.hzdr.de/infohub/hpc/storage.html">https://fwcc.pages.hzdr.de/infohub/hpc/storage.html</a>&gt;</p>
</li>
<li><p>Alternatively, simply use the following command</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>sshfs<span class="w"> </span>username@hemera5.fz-rossendorf.de:folder/file/location<span class="w"> </span>folder/location/in/local
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="horovod">
<h2>Horovod<a class="headerlink" href="#horovod" title="Permalink to this headline"></a></h2>
<div class="section" id="id2">
<h3>Hemera5 (HZDR)<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>The following suggestions are derived from tests on the slurm based cluster
Hemera5. They should be applicable to other slurm based clusters as well.
You should always use all GPUs per node in a multinode setup.</p>
<p>If you want to run a MALA job on a slurm based machine, follow this outline as submit script:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -N NUMBER_OF_NODES</span>
<span class="c1">#SBATCH --ntasks-per-node=MAX_NUMBER_OF_GPUS_PER_NODE</span>
<span class="c1">#SBATCH --gres=gpu:MAX_NUMBER_OF_GPUS_PER_NODE</span>

<span class="c1">#.... other #SBATCH options ...</span>

<span class="c1">#... module loading and such ...</span>

mpirun<span class="w"> </span>-np<span class="w"> </span>NUMBER_OF_NODES*MAX_NUMBER_OF_GPUS_PER_NODE<span class="w"> </span>-npernode<span class="w"> </span>MAX_NUMBER_OF_GPUS_PER_NODE<span class="w"> </span>-bind-to<span class="w"> </span>none<span class="w"> </span>-x<span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO<span class="w"> </span>-x<span class="w"> </span>LD_LIBRARY_PATH<span class="w"> </span>-x<span class="w"> </span>PATH<span class="w"> </span>-mca<span class="w"> </span>pml<span class="w"> </span>ob1<span class="w"> </span>-mca<span class="w"> </span>btl<span class="w"> </span>^openib<span class="w"> </span>python3<span class="w"> </span>your_mala_script.py
</pre></div>
</div>
</div></blockquote>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="external_modules.html" class="btn btn-neutral float-left" title="External modules" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tested_systems.html" class="btn btn-neutral float-right" title="Successfully tested on" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021 National Technology &amp; Engineering Solutions of Sandia, LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S. Government retains certain rights in this software. Attila Cangi, J. Austin Ellis, Lenz Fiedler, Daniel Kotik, Normand Modine, Sivasankaran Rajamanickam, Steve Schmerler, Aidan Thompson.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>